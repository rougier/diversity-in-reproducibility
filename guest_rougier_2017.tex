\documentclass[jou]{apa6}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}
\usepackage{apacite} 

% 500-1000 words by when we are done :)

\title{What is computational reproducibility?}
\shorttitle{What is computational reproducibility?}

\twoauthors{Olivia Guest}{Nicolas P. Rougier}
\twoaffiliations{Department of Experimental Psychology\\University College London, United Kingdom}{INRIA Bordeaux Sud-Ouest, Talence, France\\
Institut des Maladies Neurodégénératives, Université Bordeaux, Centre National de la Recherche Scientifique, UMR 5293, Bordeaux, France\\
LaBRI, Université de Bordeaux, Institut Polytechnique de Bordeaux, Centre National de la Recherche Scientifique, UMR 5800, Talence, France}
%olivia.guest@psy.ox.ac.uk
%nicolas.rougier@inria.fr
\abstract{}

\begin{document}
\maketitle
In our previous contribution, we proposed computational modelling-related definitions for \textbf{replicable}, i.e., experiments within a model can be recreated using its original codebase, and \textbf{reproducible}, i.e., a model can be recreated based on its specification. We stressed the importance of specifications and of access to codebases. Furthermore, we highlighted an issue in scholarly communication --- many journals do not require nor facilitate the sharing of code. In contrast, many third-party services have filled the gaps left by traditional publishers \cite<e.g.,>{binder, github, osf, rescience}. Notwithstanding, journals and peers rarely request or expect use of such services. We ended by asking: are we ready to associate codebases with articles and are we prepared to ensure computational theories are well-specified and coherently implemented?

We received a range of replies --- including proposals for: intermediate levels between replicability and reproducibility (Crook, Hinsen); going beyond reproducibility (Kidd); encompassing computational science at large (Gureckis \& Rich, Varoquaux); and addressing communities as a function of expertise (French \& Addyman).
On the one hand, some focus on broader issues of evaluation, on empirical data collection, or on software engineering.
On the other hand, some contributions identify specific steps during the process of evaluating modelling accounts. 
We will touch on the former first.

In the contribution by Varoquaux, reproducibility is defined more broadly, encompassing replicability, e.g., to include code rot \cite<e.g.,>{Eklund12072016}.
However, the titular computational reproducibility is orthogonal to maintaining a re-usable codebase.
Software and hardware inevitably go out of fashion causing codebases to expire.
However, the ideas encapsulated by modelling software, the overarching theory, could withstand the effects of entropy if specified coherently.
% see https://en.wikipedia.org/wiki/Software_entropy
For example, early artificial neural network codebases are not required for junior researchers to understand the basics.
Indubitably, there is a balance to be struck between how much to reimplement previous work and how much to stand on the shoulders of other modellers and their code. 

Gureckis and Rich zoom out to discuss the empirical replication crisis in psychology.
They provide definitions for \textit{reproducible}, an experiment that can be followed exactly, and \textit{replicable}, an experiment that is reproducible but also requires specific data collection conditions to be met.
Implicit knowledge on the running of experiments often goes unpublished and thus only fully automated experiments run on-line are computationally reproducible psychology.

Epistemically, empirical, as well as software, replication and reproduction are distinct from their modelling namesakes.
They can be seen as six related endeavours.
The difference between software for science (e.g., code used to run a statistical test on a dataset) and science that is software (e.g., codebase for a cognitive model) is an important one to underline. 
In the former case the code is a tool, in the latter it constitutes an experiment.
Notwithstanding, all such evaluations have scientific merit.

\section{Levels of Evaluation}
We mentioned by name two of the levels of evaluating modelling work --- however some of the replies mention further details and checks.
Consensus is that replicability constitutes the bare minimum.
For example, Hinsen separates the steps at which a model can be taken as dependable into three levels of trust: that authors have not introduced bugs, that the model is reproducible as presented, and that it is congruent with empirical data and describing a real-world phenomenon.
These levels roughly map onto the three levels of talking about modelling work more generally as Kidd also notes \cite<e.g.,>{marr82}: the implementation, model, and theory levels.  

\subsection{Implementation Level}
With respect to the implementation level, the codebase itself, there are more gradations of evaluations to carry out.
As mentioned by Crook: re-running code both within a lab and by others allows for checking for some types of bugs and if assumed-to-be-irrelevant variables, e.g., the random seed, are indeed not driving any of the results.
This also ensures documentation is appropriate. 
Successes at this level indicate a model is \textit{replicable}.

\subsection{Model Level}
To evaluate the quality of the specification, we may re-write the model based on it --- what we termed \textit{reproducible}.
This forms the bedrock of the scientific evaluation of such accounts, providing substantial evidence for or against depending on the reimplementation's success.
As Kidd mentions, and as the authors themselves have discovered \cite{cooper14}, during this process we can: discern when implementation details must be elevated to the theory level and vice versa; the evaluate the specification; and locate conceptual bugs.

\subsection{Theory Level}
Science encompasses many methods for evaluating theories and the hypotheses they generate.
One way is to implement a theory computationally.
Another is to test predictions by gathering empirical data.
As Crook points out, this data is also used to evaluate (validate, train, etc.) modelling accounts of the phenomenon of interest --- it (in)forms part of the theory.
In such cases, data requires re-collecting to further test a model's generalisability.
As such, data would need to be associated with the original article and codebase. 
If the phenomenon to-be-modelled, Hinsen warns, does not occur as described by the overarching theoretical account, e.g., if appropriate patterns in the data are not found in follow-up experiments, then both theory and model are brought into question.
This was highlighted by \citeA[p.~426]{Minsky:1965}: ``to an observer B, an object A* is a model of an object A to the extent that B can use A* to answer questions that interest [them] about A.''
%do we need a quote here? 

\section{Conclusion}
The breadth of answers we received show that reproducibility can be interpreted in many ways both depending on domain and level of analysis.
Even though definitions across the replies do not fully converge, we all agree that change is both needed and imminent.

In the contribution by French and Addyman, sharing specifications is seen as less important than we and others envisage, especially with respect to the audience.
We see an inherent risk in presenting models without some understanding of the computations.
Importantly however, we do agree on some fundamentals: that sharing the codebase for models is invaluable; that journal articles should be linked with their original codebase; and that the principles and motivations behind reproducing computational models  is useful \citeA<as practiced by e.g.,>{rescience}

For example, in answer to the question we posed: Hinsen proposes that modellers should detail how models have been and can be verified, by including a section on verification in our articles; while Crook refers to some invaluable community-driven initiatives with which to share codebases and specifications. 
We hope that modelling-focused labs make use of the tools listed to disseminate their work; and that eventually some consensus on how to do this effectively is reached.

Ultimately, Crook hopes as we do for top-down publisher-enforced sharing of these resources in partially-centralised on-line repositories.
All modellers have a prominent role to play and we should start to make change actually happen.

\bibliographystyle{apacite}
\bibliography{ref}
% The following space works around a bug in typesetting the references, where the hanging indent of the last reference is incorrectly set.
\hspace*{1cm}
\end{document}
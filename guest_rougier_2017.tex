\documentclass[jou]{apa6}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}
\usepackage{apacite} 


% 500-1000 words by the 10th of January

\title{What is computational reproducibility?}
\shorttitle{What is computational reproducibility?}

\twoauthors{Olivia Guest}{Nicolas P. Rougier}
\twoaffiliations{Department of Experimental Psychology\\University College London, United Kingdom}{INRIA Bordeaux Sud-Ouest, Talence, France\\
Institut des Maladies Neurodégénératives, Université Bordeaux, Centre National de la Recherche Scientifique, UMR 5293, Bordeaux, France\\
LaBRI, Université de Bordeaux, Institut Polytechnique de Bordeaux, Centre National de la Recherche Scientifique, UMR 5800, Talence, France}
%olivia.guest@psy.ox.ac.uk
%nicolas.rougier@inria.fr
\abstract{}

\begin{document}
\maketitle
%Recap of previous 

%\section{Introduction}
In our previous contribution, we proposed computational modelling-related definitions for \textbf{replicable}, i.e., experiments within a model can be recreated using its original codebase, and \textbf{reproducible}, i.e., a model can be recreated based on its specification. We stressed the importance of specifications and of access to codebases. Furthermore, we highlighted an issue in scholarly communication --- many journals do not require nor facilitate the sharing of code. In contrast, many third-party services have filled the gaps left by traditional publishers \cite<e.g.,>{binder, github, osf, rescience}. Notwithstanding, journals and peers rarely request or expect use of such services. We ended by asking: are we ready to associate codebases with articles and are we prepared to ensure computational theories are well-specified and coherently implemented?

% Proposed final structure:
\section{Levels of Evaluation}
All the ways that have been mentioned for evaluating modelling work. 

\section{Proposed Solutions}
All the solutions proposed to fix lack of: code and specification sharing and evaluation.

\section{Conclusion}
Future directions and feasibility of changes and solutions given previously.
%end of proposed structure

% Olivia Notes — Not the actual final product, just notes.
% Feel free to read, but please don't edit my notes.
% Although add comments in LaTeX if you really want to. :)
% Start of Olivia's notes.
\section{Crook}
Crook mentions detailed levels at which modelling work can be evaluated scientifically, i.e., re-running and re-writing code both within a lab and by others, which can be seen as falling on the spectrum between replication and reproduction. She also refers to some invaluable community-driven initiatives and tools in which to share codebases and specifications.

Crook draws attention to the matter of empirical data used to validate, train, etc., models. Such data (in)forms part of the theory of the modelling account. Thus data would need re-collecting in order to further test a model's generalisability --- and it would need to be associated with the original article and codebase. 

Ultimately, she hopes (as we do) for top-down publisher-enforced sharing of these resources in partially-centralised on-line repositories.

\section{Kidd}
Kidd rightfully draws attention to the process we have dubbed reproduction and she reimplementation --- and in fact directly addressing the titular question: what is computational reproducibility? Re-writing modelling work from scratch based on the specification forms the bedrock of the scientific evaluation of such accounts, providing substantial evidence for or against depending on the reimplementation's success. As Kidd mentions, and the authors themselves have discovered \cite{cooper14}, this process can enlighten us with respect to: bugs, when implementation details must be elevated to the theory level and vice versa, and more generally be pedagogically useful and even enjoyable.

\section{Hinsen}
Hinsen analyses in more detail the levels at which a model can be verified and thus can be trusted. He outlines three such levels of trust: that authors have not introduced bugs, that the model is reproducible as presented, and that it is congruent with empirical data and describing a real-world phenomenon. Hinsen proposes that modellers should detail how models have been and can be verified, by including a section on verification in our articles.

\section{Varoquaux}
Varoquaux defines reproducibility differently, perhaps more broadly, to us. However we definitely empathise with the problems of code rot, which falls under what we define as replicability. Notwithstanding, the titular computational reproducibility is far more scientifically pertinent than maintaining a re-usable codebase. Software and hardware inevitably goes out of fashion causing codebases to expire, but the ideas encapsulated by a model, the overarching theory, could withstand the effects of time if specified coherently. For example, the code used for the first neural network models is not required in order for a junior researcher to get to grips with the basics. In fact reimplementing previous modelling work has pedagogical benefits as well as scientific ones. Indubitably, there is a delicate balance to be struck between how much to focus on reimplementing models and standing on the shoulders of previous modellers. 

\section{Gureckis and Rich}
Gureckis and Rich zoom out to discuss the empirical, i.e., not directly modelling-related, experimental replication crisis in psychology. The provide their own definitions for reproducible, which is an experiment that can be followed exactly, and replicable, which is an experiment They explain that implicit knowledge used in the running of experiments often goes unmentioned in journal articles. 

\section{French and Addyman}
French and Addyman

% End of Olivia's notes.

% Start of Nicolas's notes

\section{Varoquaux}
G. Varoquaux raised an important point by explaining reproducibility is not
enough and that software reuse is an important goal in the scientific
process. While we agree with him, we think however that these two goals are
rather orthogonal and achieving one does not exempt you from achieving the
other. A dramatic illustration has been made 2 years ago with the so-called
Heartbleed incident (a security bug in the OpenSSL cryptography library). While
the OpenSSL library is present in two-thirds (estimation) of websites, nobody
spot the bug for two years mostly because the libray is maintained by only a
few people (with only one full-time developer). This is a clear case of great
re-usability without the correctness counter-part. Even more recently, a bug
has been discovered in a library that is used by many fMRI software. This is
where G. Varoquaux's explanation about unit tests to ensure the quality of a
program make even more sense.

\section{Hinsen}
In his answer, K.Hinsen distinguishes three different levels of trust for
sharing computational levels. The first one (the "easy" one) is what we defined
as replicability and could be possibly soon automated in the near future. The
second level, the one we defined as reproducibility, is more difficult to
address according to K. Hinsen because of the inner complexity of some models
that cannot be fully described or described in a consistent way. Furthermore,
the possibly very long time of development for some software may make
reproducibility out of reach. Finally, the third and last level identified by
K. Hinsen is probably the one we did not underline enough in our short dialogue
initiation. Science is about models, not software (Note: sounds a bit hard said
like that).

\section{Gureckis}
Gureckis \& Rich draw an insightful parallel between reproducibility in
computational science and reproducibility in psychology. In face of the recent
reproducibility crisis in psychology, they propose to address the frequent
problem of incomplete method section using a computationally reproducible
experiments based on web technology. Even if such proposal cannot satisfies
each and every experiment, this is, in our opinion, an interesting use of
computational technology that promotes actual reproducibility in an
experimental domain.

\section{Crook}
Crook proposed a gradation that goes from mere replication to the full
independent reproduction and definine two intermediate levels. Furthermore, she
underlines the existence of model repositories even though they gather only a
tiny part of the scientific activity in this domain. More importantly, in the
case of neuroscience, she reminds us of the PyNN initiative that allow for a
simulator-agnostic specification of a model and the NeuroML language for a more
formal description. These initiatives partially solves the reproducibility
problem we highlighted even though you have to constrain the model in the
proposed paradigm (PyNN or NeuroML).

\section{Kidd}
Celeste Kidd introduces the notion of conceptual replication that goes beyond
mere reproduction as we've defined it. On a sidenote, may it is important to
re-explain here that we may have misused (inverted actually) replication and
reproduction as explained by L.Barba in her blog post (url ?). Conceptual
replication means to try to re-implement the main idea of a model, possibly
taking a totally novel approach but still getting the same qualitative
result. Based on Daid Marr three-levels approach, she propose to encompass the
implementation level in order to guarantee the results. This would offer a
powerful confirmation if the original work but it come at the price of
extensive effort in this conceptual replication.

\section{French}
French \& Addyman, not done yet.

% End of Nicolas's notes

\bibliographystyle{apacite}
\bibliography{ref}
% The following space works around a bug in typesetting the references, where the hanging indent of the last reference is incorrectly set.
\hspace*{1cm}
\end{document}